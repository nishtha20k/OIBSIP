# -*- coding: utf-8 -*-
"""TASK 4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fyMOOzlGKBgIxR1rHBl_E_kj6BbBMOtC

# **Libraries**
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

"""# **Dataset , it's cleaning and analysis**"""

df = pd.read_csv('spam.csv', encoding="latin-1", usecols=["v1","v2"])
df

df.rename(columns={"v1": "Category","v2": "Message"}, inplace = True)
df

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df["Category"] = le.fit_transform(df["Category"])
df
# ham = 0 , spam = 1

df.shape

df.info()

df.isnull().sum()

df.duplicated().sum()

df = df.drop_duplicates(keep="first")

df.shape

df["Category"].value_counts()

plt.pie(df["Category"].value_counts(),autopct = "%.2f",labels=['Ham','Spam'])
plt.show()

class_counts = df['Category'].value_counts()
plt.bar(class_counts.index, class_counts.values)
plt.xlabel('Class')
plt.ylabel('Number of Messages')
plt.title('Class Distribution (Spam vs. Ham)')
plt.show()

from wordcloud import WordCloud, STOPWORDS
comment_words = ''
stopwords = set(STOPWORDS)

for val in df.Message:
    val = str(val)
    tokens = val.split()

    for i in range(len(tokens)):
        tokens[i] = tokens[i].lower()

    comment_words += " ".join(tokens)+" "

wordcloud = WordCloud(width = 800, height = 600,background_color ='#f8f8ff', stopwords = stopwords, min_font_size = 10).generate(comment_words)

plt.figure(figsize = (5,5), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)
plt.show()

"""# **Model Training**"""

x = df['Message']

y = df["Category"]

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=42)

from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer()
vectorizer.fit(x_train)

X_train_cv = vectorizer.transform(x_train)
X_test_cv = vectorizer.transform(x_test)

from sklearn.linear_model import LogisticRegression
from sklearn import metrics

logreg = LogisticRegression()
logreg.fit(X_train_cv, y_train)

y_pred = logreg.predict(X_test_cv)
print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test_cv, y_test)))

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 score:", f1)

"""# **Prediction with custom words**"""

# Importing MultinomialNB
from sklearn.naive_bayes import MultinomialNB

# Using MultinomialNB for prediction Custom words
classifier = MultinomialNB()
classifier.fit(X_train_cv, y_train)

custom_word = "Offer for internship"
custom_word_vec = vectorizer.transform([custom_word])
prediction = classifier.predict(custom_word_vec)[0]

if prediction == 1:
    prediction = "Spam"
else:
    prediction = "Ham"

print(f"Custom word '{custom_word}' is predicted as: {prediction}")